{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дополнительные задачи ответы в конце"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 1 Что будет на выходе? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = [(1, 2, 3), (1, 2), (3, 4), (0, 1, 2, 3, 4)]\n",
    "s = 0\n",
    "for a, *b in recs:\n",
    "    s += sum(b)\n",
    "#print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2 Что будет на выходе? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[0]]\n",
    "x = x + x*2\n",
    "x[0].append(1)\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вопросы\n",
    "- 1 Как работает логистическая регрессия?\n",
    "- 2 Почему оптимизация логлоса приводит к оценке вероятностей?\n",
    "- 3 Как работает градиентный бустинг? А в задаче классификации? \n",
    "- 4 Вопросы про глубину и количество деревьев в GBM и RF? \n",
    "- 5 Чем хорош и плох ROC-AUC? Насколько он уместен с несбалансированными выборками? А в задачах Ранжирования? \n",
    "- 6 Как оценивать статзначимость в А/В?\n",
    "\n",
    "\n",
    "Ответ дается в краткой форме + ссылка на дополнительные материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Глубина и количество деревьев в GBM и RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Gradient Boosting Model\n",
    "\n",
    "Градиентный Бустинг - это метод машинного обучения, используемый, в частности, в задачах регрессии и классификации. Он дает модель предсказания **в виде ансамбля слабых моделей предсказания**, которые обычно являются деревьями решений. Когда дерево решений является слабым обучаемым, результирующий алгоритм называется gradient-boosted trees; он обычно превосходит random forest. Модель gradient-boosted trees строится поэтапно, как и в других методах бустинга, но она обобщает другие методы, позволяя оптимизировать произвольную дифференцируемую функцию потерь.\n",
    "\n",
    "### Кратко: Много (тысячи) деревьев небольшой глубины (до 10). \n",
    "\n",
    "### Ниже приведены отрывки из [Градиентный бустинг - Учебник по ML от ШАД ](https://ml-handbook.ru/chapters/grad_boost/intro)\n",
    "\n",
    "\n",
    "Типичный градиентный бустинг имеет в составе **несколько тысяч деревьев решений**, которые необходимо строить последовательно. Построение решающего дерева на выборках типичного размера и современном железе, даже с учетом всех оптимизаций, требует небольшого, но всё-таки заметного времени (0.1-1c), которое для всего ансамбля превратится в десятки минут. Это не так быстро, как обучение линейных моделей, но всё-таки значительно быстрее, чем обучение типичных нейросетей.\n",
    "\n",
    "### Темп обучения (learning rate)\n",
    "\n",
    "Обучение композиции с помощью градиентного бустинга может привести к переобучению, если базовые алгоритмы слишком сложные. Например, **если сделать решающие деревья слишком глубокими (более 10 уровней)**, то при обучении бустинга ошибка на обучающей выборке даже при довольно скромном K может приблизиться к нулю, то есть предсказание будет почти идеальным, но на тестовой выборке всё будет плохо.\n",
    "\n",
    "Существует два решения этой проблемы. Во-первых, необходимо упростить базовую модель, **уменьшив глубину дерева** (либо примерив какие-либо ещё техники регуляризации). Во-вторых, мы можем ввести параметр, называемый темпом обучения (learning rate). Присутствие этого параметра означает, что каждый базовый алгоритм вносит относительно небольшой вклад в итоговое решение.\n",
    "\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "Отдельные деревья решений можно легко интерпретировать, просто визуализируя их структуру. Но в модели градиентного бустинга содержатся сотни деревьев, и поэтому её нелегко интерпретировать путем визуализации входящих в неё деревьев. При этом хотелось бы, как минимум, понимать, какие именно признаки в данных оказывают наибольшее влияние на предсказание композиции.\n",
    "\n",
    "Можно сделать следующее наблюдение: признаки, используемые в верхней части дерева, влияют на окончательное предсказание для большей доли обучающих объектов, чем признаки, попавшие на более глубокие уровни. Таким образом, ожидаемая доля обучающих объектов, для которых происходило ветвление по данному признаку, может быть использована в качестве оценки его относительной важности для итогового предсказания. Усредняя полученные оценки важности признаков по всем решающим деревьям из ансамбля, можно уменьшить дисперсию такой оценки и использовать ее для отбора признаков.\n",
    "\n",
    "\n",
    "### Где используется\n",
    "\n",
    "Везде :) На сегодня градиентный бустинг – это, фактически, один из двух подходов, которые используются на практике (второй – это нейронные сети, конечно). Он формально слабее и менее гибок, чем сети, но выигрывает в простоте настройки темпа обучения и применения, размере и интерпретируемости модели.\n",
    "\n",
    "Во многих компаниях, так или иначе связанных с ML, он используется для всех задач, которые не связаны с однородными данными (картинками, текстами, etc). Типичный поисковый запрос в Яндексе, выбор отеля на Booking.com или сериала на вечер в Netflix задействует несколько десятков моделей GBDT.\n",
    "\n",
    "Впрочем, в будущем можно ожидать плавного исчезновения этого подхода, так как улучшение архитектур глубинного обучения и дальнейшее развитие железа нивелирует его преимущество по сравнению с нейросетями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайные леса или случайные леса решений - это метод ансамблевого обучения (**Bagging**) для классификации, регрессии и других задач, который работает путем построения множества деревьев решений во время обучения.\n",
    "-  Для задач классификации выходным результатом случайного леса является класс, выбранный большинством деревьев.\n",
    "-  Для задач регрессии возвращается средний или усредненный прогноз отдельных деревьев. \n",
    "\n",
    "Случайные леса решений исправляют привычку деревьев решений чрезмерно подстраиваться под обучающий набор. Случайные леса в целом превосходят деревья решений, но их точность ниже, чем у деревьев с градиентным усилением. Однако характеристики данных могут влиять на их производительность.\n",
    "\n",
    "### Кратко: Регулируем число деревьев на обучаемой выборке, деревья строим глубокие.\n",
    "\n",
    "\n",
    "### Ниже приведены отрывки из [Ансамбли в машинном обучении - Учебник по ML от ШАД ](https://ml-handbook.ru/chapters/ensembles/intro#random-forest)\n",
    "\n",
    "### Какая должна быть глубина деревьев в случайном лесе?\n",
    "Ошибка модели (на которую мы можем повлиять) состоит из смещения и разброса. Разброс мы уменьшаем с помощью процедуры бэггинга. На смещение бэггинг не влияет, а хочется, чтобы у леса оно было небольшим. Поэтому смещение должно быть небольшим у самих деревьев, из которых строится ансамбль. У неглубоких деревьев малое число параметров, то есть дерево способно запомнить только верхнеуровневые статистики обучающей подвыборки. Они во всех подвыборках будут похожи, но будут не очень подробно описывать целевую зависимость. Поэтому при изменении обучающей подвыборки предсказание на тестовом объекте будет стабильным, но не точным (низкая дисперсия, высокое смещение). Наоборот, у глубоких деревьев нет проблем запомнить подвыборку подробно. Поэтому предсказание на тестовом объекте будет сильнее меняться в зависимости от обучающей подвыборки, зато в среднем будет близко к истине (высокая дисперсия, низкое смещение). \n",
    "\n",
    "**Вывод: используем глубокие деревья.**\n",
    "\n",
    "### Сколько должно быть деревьев в случайном лесе?\n",
    "Выше было показано, что увеличение числа элементарных алгоритмов в ансамбле не меняет смещения и уменьшает разброс. Так как число признаков и варианты подвыборок, на которых строятся деревья в случайном лесе, ограничены, уменьшать разброс до бесконечности не получится. \n",
    "\n",
    "**Вывод 1: Поэтому имеет смысл построить график ошибки от числа деревьев и ограничить размер леса в тот момент, когда ошибка перестанет значимо уменьшаться.**\n",
    "\n",
    "Вторым практическим ограничением на количество деревьев может быть время работы ансамбля. Однако есть положительное свойство случайного леса: случайный лес можно строить и применять параллельно, что сокращает время работы, если у нас есть несколько процессоров. Но процессоров, скорее всего, всё же сильно меньше числа деревьев, а сами деревья обычно глубокие. \n",
    "\n",
    "**Вывод 2: Поэтому на большом числе деревьев Random Forest может работать дольше желаемого и количество деревьев можно сократить, немного пожертвовав качеством.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 ROC-AUC.  Плюсы и минусы. Несбалансированные выборки. Задача ранжирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 ROC-AUC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кратко\n",
    "ROC - кривая характеризующая модели классификации (при изменении порога чувствительности модели threshold), которые возвращают \"оценку\" = вероятность принадлежности классу {0,1}.\n",
    "\n",
    "ПЛЮСЫ: позволяет визуально сравнить эффективность разных моделей и охватывают несколько аспектов классификации ( ложно положительные и ложно отрицательные результаты)\n",
    "\n",
    "более подробнее надо дать определение: \n",
    " - порог чувствительности модели threshold (Если значение в модели = вероятность > threshold, то классифицируем как 1 = ожирение, спам, мошенник)\n",
    " - доля истинно положительных результатов (True Positive Rate) TPR = TP/(TP+FN)\n",
    " - доля ложно положительных результатов (False Positive Rate)  FRP = FP/(FP+TN) \n",
    "\n",
    "AUC - area under curve, \"величина\" от 0 до 1 позволяющая, наиболее быстро сравнивать ROC кривые разных моделей. \n",
    "- Когда AUC выше 0.5 значит модель предсказывает лучше, чем предсказывать случайным выбором (AUC=0.5).\n",
    "- Когда AUC ниже 0.5 значит, с моделью что-то не так.\n",
    "\n",
    "\n",
    " Источники:\n",
    " -  [StatQuest](https://www.youtube.com/watch?v=4jRBRDbJemM&t=19s)\n",
    " - [Александр Дьяконов: AUC ROC (площадь под кривой ошибок)](https://dyakonov.org/2017/07/28/auc-roc-%D0%BF%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C-%D0%BF%D0%BE%D0%B4-%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Несбалансированные выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кратко: \n",
    "Задачи с дисбалансом чаще всего возникают, когда какой-то из классов соответствует очень редко наблюдаемым или диагностируемым явлениям (дефолт, поломка, редкая болезнь, мошенничество и т.п.)\n",
    "\n",
    "Причины дисбалансов бывают разные и им соответствуют разные механики решения. \n",
    "\n",
    "Если у нас обычная бинарная задача классификации, пропорции классов не меняются со временем и процент объектов класса 1 – от 2% до 10%. Эта ситуация соответствует, например, задаче банковского скоринга в стабильной экономической обстановке.\n",
    "\n",
    "Ключевой проблемой в данном случае является выбор порога чувствительности модели threshold. \n",
    "\n",
    "\n",
    "### Дополнительно:\n",
    "- [Александра Дьяконова: Дисбаланс классов](https://dyakonov.org/2021/05/27/imbalance/)\n",
    "- [Jason Brownlee: ROC Curves and Precision-Recall Curves for Imbalanced Classification](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Задача ранжирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто встречаются задачи, в которых целевой признак по-прежнему бинарный,\n",
    "но при этом необходимо ранжировать объекты, а не просто предсказывать их класс.\n",
    "Например, в задаче предсказания реакции клиента можно выдавать сортированный\n",
    "список, чтобы оператор мог в первую очередь позвонить клиентам с наибольшей\n",
    "вероятностью положительного отклика.\n",
    "Поскольку многие алгоритмы возвращают\n",
    "вещественный ответ b(x), который затем бинаризуется по порогу t, то можно просто\n",
    "сортировать объекты по значению b(x). \n",
    "\n",
    "Критерий AUC-ROC имеет большое число интерпретаций — например, его можно использовать как вероятности того, что случайно выбранный положительный объект окажется выше случайно выбранного отрицательного объекта в ранжированном списке, порожденном b(x).\n",
    "\n",
    "### Источник:\n",
    "[Евгений Соколов: Семинары по выбору моделей](http://www.machinelearning.ru/wiki/images/1/1c/sem06_metrics.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Статистическая значимость в А/В тестах. Как оценивать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "У Вас сейчас применяются параметры А, а вы хотите понять, а что если попробовать параметры В?\n",
    "Вы провели замеры и получили:\n",
    " - при параметрах A: 11 \"успеха\" из 100 \"попыток\"\n",
    " - при параметрах В: 13 \"успеха\" из 100 \"попыток\"\n",
    "\n",
    "[Evan Miller calculator](https://www.evanmiller.org/ab-testing/chi-squared.html#!11/100;13/100@95)\n",
    "\n",
    "А если бы Вы получили:\n",
    " - при параметрах A: 110 \"успеха\" из 1000 \"попыток\"\n",
    " - при параметрах В: 130 \"успеха\" из 1000 \"попыток\"\n",
    "\n",
    "А если бы Вы получили:\n",
    " - при параметрах A: 1100 \"успеха\" из 10000 \"попыток\"\n",
    " - при параметрах В: 1300 \"успеха\" из 10000 \"попыток\"\n",
    "\n",
    "[Evan Miller calculator](https://www.evanmiller.org/ab-testing/chi-squared.html#!1100/10000;1300/10000@95)\n",
    "\n",
    "\n",
    "Вопрос: В каком из экспериментов можно утверждать, что теперь надо перейти на параметры В?\n",
    "\n",
    "Ответ: Мы должны быть уверены, что результаты эксперимента не случайны, то есть, что параметры B будут приносить больше \"успеха\", чем параметры А \"регулярно\".\n",
    "\n",
    "Для этого вводят и используют **Уровень статистической значимости** (α). Это уровень риска, который вы принимаете при ошибках первого рода (отклонение нулевой гипотезы, если она верна), обычно α = 0.05. Это означает, что в 5% случаев вы будете обнаруживать разницу между A и B, которая на самом деле обусловлена случайностью. Чем ниже выбранный вами уровень значимости, тем ниже риск того, что вы обнаружите разницу, вызванную случайностью.\n",
    "\n",
    "\n",
    "На результат и обоснованность экспериментов влияют:\n",
    " - масштабные изменения в среде (новостные события, природные катаклизмы, распродажа у конкурентов),\n",
    " - предопределенность экспериментатора (bias) и ошибки проведения эксперимента.\n",
    "\n",
    "\n",
    "Используем слишком высокий - тесты возможных улучшений будут проваливаться, хотя улучшения на самом деле есть. Используем слишком низкий - часто будем получать \"подтверждения\" ложных улучшений.\n",
    "\n",
    "Независимо от выбранного уровня значимости, принимая решения по результатам AB-тестов, время от времени мы будем ошибаться и наносить ущерб бизнесу. Выбирая уровень статистической значимости тестов, мы можем ограничить количество ошибок и балансировать между пользой от оправданно успешных экспериментов и ущербом от ошибочно успешных.\n",
    "\n",
    "### Дополнительно:\n",
    "\n",
    "[academy.yandex.ru: Как провести A/B-тестирование: 6 простых шагов](https://academy.yandex.ru/posts/kak-provesti-a-b-testirovanie-6-prostykh-shagov),\n",
    "\n",
    "[Habr: Как выбрать уровень статистической значимости для AB-теста и как интерпретировать результат](https://habr.com/ru/post/554194/),\n",
    "\n",
    "[Product Analyst из Яндекс: \"Разбираемся с нуля в А/Б-тестах\"](https://productstar.ru/ab-test) или [видео](https://youtu.be/F3yGiFY9Nmo?t=3961).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ответы на задачи "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  1\n",
      "b =  [2, 3]\n",
      "--------------\n",
      "a =  1\n",
      "b =  [2]\n",
      "--------------\n",
      "a =  3\n",
      "b =  [4]\n",
      "--------------\n",
      "a =  0\n",
      "b =  [1, 2, 3, 4]\n",
      "--------------\n",
      "s =  21\n"
     ]
    }
   ],
   "source": [
    "recs = [(1, 2, 3), (1, 2), (3, 4), (0, 1, 2, 3, 4)]\n",
    "s = 0\n",
    "for a, *b in recs:\n",
    "    print('a = ',a)\n",
    "    print('b = ',b)\n",
    "    print('--------------')\n",
    "    s += sum(b)\n",
    "print('s = ',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[0]]\n",
      "x*2 =  [[0], [0]]\n",
      "after x = x + x*2\n",
      "x =  [[0], [0], [0]]\n",
      "x[0] =  [0]\n",
      "after x[0].append(1)\n",
      " [[0, 1], [0, 1], [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "x = [[0]]\n",
    "print('x = ',x)\n",
    "print('x*2 = ',x*2)\n",
    "x = x + x*2\n",
    "print('after x = x + x*2\\nx = ',x)\n",
    "print('x[0] = ', x[0])\n",
    "x[0].append(1)\n",
    "print('after x[0].append(1)\\n',x)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c3cd5798e9bc5ac0f2ed63b597e52e57a4fe4f1b0d3c3e2531833f17d2923d8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
